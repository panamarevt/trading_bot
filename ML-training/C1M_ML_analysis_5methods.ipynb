{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle # to save and load pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_curr', 'symbol', 'price', 'pattern', 'origin', 'ranging',\n",
       "       'd_ranging', 'lower', 'upper', 'middle', 'd_lower', 'd_upper',\n",
       "       'd_middle', 'ema_10', 'ema_200', 'd_ema_10', 'd_ema_200', 'k_15',\n",
       "       'd_15', 'd_k_15', 'd_d_15', 'stoch_cond_5', 'candle_color', 'volume',\n",
       "       'ntrades', 'tb_volume', 'vol_inc', 'dist_to_BB', 'profit', 'elapsed',\n",
       "       'min_price', 'max_price', 'profit_s15', 'elapsed_s15', 'min_price_s15',\n",
       "       'max_price_s15', 'Unnamed: 36'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df = pd.read_csv('buy_signals_backtest_ML_July-Aug2020_volume.dat')\n",
    "trades_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profitability = [0 if item < 0 else 1 for item in trades_df['profit']]\n",
    "trades_df['profitability'] = profitability\n",
    "profitability_s15 = [0 if item < 0 else 1 for item in trades_df['profit_s15']]\n",
    "trades_df['profitability_s15'] = profitability_s15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 'time_curr' to `datetime` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_curr', 'symbol', 'price', 'pattern', 'origin', 'ranging',\n",
       "       'd_ranging', 'lower', 'upper', 'middle', 'd_lower', 'd_upper',\n",
       "       'd_middle', 'ema_10', 'ema_200', 'd_ema_10', 'd_ema_200', 'k_15',\n",
       "       'd_15', 'd_k_15', 'd_d_15', 'stoch_cond_5', 'candle_color', 'volume',\n",
       "       'ntrades', 'tb_volume', 'vol_inc', 'dist_to_BB', 'profit', 'elapsed',\n",
       "       'min_price', 'max_price', 'profit_s15', 'elapsed_s15', 'min_price_s15',\n",
       "       'max_price_s15', 'Unnamed: 36', 'profitability', 'profitability_s15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df['time_curr'] = pd.to_datetime(trades_df['time_curr'])\n",
    "\n",
    "# Sort the dataframe by the date:\n",
    "trades_df.sort_values(by=['time_curr'], inplace=True)\n",
    "trades_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check what was the profitability during backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trade: 2020-07-02 00:00:00, end trade: 2020-10-25 11:03:00 \n",
      "Total profitability in 115 days 11:03:00:  0.6163383874167493\n",
      "total n trades:  14114\n",
      "Total profitability in 115 days 11:03:00 with 0.15 stop loss:  0.4427518775683718\n",
      "total n trades:  14114\n"
     ]
    }
   ],
   "source": [
    "time_span = trades_df.time_curr.iloc[-1] - trades_df.time_curr.iloc[0]\n",
    "print(f\"Start trade: {trades_df.time_curr.iloc[0]}, end trade: {trades_df.time_curr.iloc[-1]} \")\n",
    "print(f\"Total profitability in {time_span}: \", trades_df['profitability'].sum()/len(trades_df['profitability']) )\n",
    "print(\"total n trades: \", len(trades_df['profitability']))\n",
    "print(f\"Total profitability in {time_span} with 0.15 stop loss: \", trades_df['profitability_s15'].sum()/len(trades_df['profitability_s15']) )\n",
    "print(\"total n trades: \", len(trades_df['profitability_s15']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless column\n",
    "trades_df.drop(['Unnamed: 36'], axis=1, inplace=True)\n",
    "# Fill Nans\n",
    "trades_df.fillna(-9999, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "trades_df = trades_df[trades_df.d_ema_200 != -9999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we start preparing the dataset to train a model using various Machine Learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'pattern', 'origin', 'ranging', 'd_ranging', 'lower', 'upper',\n",
       "       'middle', 'd_lower', 'd_upper', 'd_middle', 'ema_10', 'ema_200',\n",
       "       'd_ema_10', 'd_ema_200', 'k_15', 'd_15', 'd_k_15', 'd_d_15',\n",
       "       'candle_color', 'volume', 'ntrades', 'tb_volume', 'dist_to_BB'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features:\n",
    "X = trades_df.drop(['time_curr','symbol','profit',\n",
    "                    'stoch_cond_5','vol_inc',\n",
    "       'elapsed', 'min_price', 'max_price', 'profit_s15', 'elapsed_s15',\n",
    "       'min_price_s15', 'max_price_s15',  'profitability',\n",
    "       'profitability_s15'], axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale small values by a factor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['price','lower','upper','middle','d_lower', 'd_upper', 'd_middle',\n",
    "            'ema_10', 'ema_200','d_ema_10', 'd_ema_200' ]: X[col] = X[col]*1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>ranging</th>\n",
       "      <th>d_ranging</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>middle</th>\n",
       "      <th>d_lower</th>\n",
       "      <th>d_upper</th>\n",
       "      <th>d_middle</th>\n",
       "      <th>ema_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_to_BB</th>\n",
       "      <th>pattern_Bullish eng.</th>\n",
       "      <th>pattern_Doji</th>\n",
       "      <th>pattern_Hammer</th>\n",
       "      <th>pattern_Harami</th>\n",
       "      <th>pattern_no</th>\n",
       "      <th>origin_lower</th>\n",
       "      <th>origin_upper</th>\n",
       "      <th>candle_color_green</th>\n",
       "      <th>candle_color_red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11666</td>\n",
       "      <td>2.241</td>\n",
       "      <td>5.444</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.343</td>\n",
       "      <td>2.284</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.263</td>\n",
       "      <td>...</td>\n",
       "      <td>1.932</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11668</td>\n",
       "      <td>2.237</td>\n",
       "      <td>5.599</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2.220</td>\n",
       "      <td>2.340</td>\n",
       "      <td>2.280</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.258</td>\n",
       "      <td>...</td>\n",
       "      <td>1.924</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10180</td>\n",
       "      <td>4.023</td>\n",
       "      <td>4.460</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>3.926</td>\n",
       "      <td>4.110</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.025</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>4.023</td>\n",
       "      <td>4.460</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>3.926</td>\n",
       "      <td>4.110</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.025</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11670</td>\n",
       "      <td>2.209</td>\n",
       "      <td>5.350</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.191</td>\n",
       "      <td>2.304</td>\n",
       "      <td>2.248</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.222</td>\n",
       "      <td>...</td>\n",
       "      <td>1.752</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price  ranging  d_ranging  lower  upper  middle  d_lower  d_upper  \\\n",
       "11666  2.241    5.444      0.011  2.226  2.343   2.284   -0.001     -0.0   \n",
       "11668  2.237    5.599      0.010  2.220  2.340   2.280   -0.000     -0.0   \n",
       "10180  4.023    4.460     -0.014  3.926  4.110   4.018    0.001     -0.0   \n",
       "405    4.023    4.460     -0.014  3.926  4.110   4.018    0.001     -0.0   \n",
       "11670  2.209    5.350      0.011  2.191  2.304   2.248   -0.000     -0.0   \n",
       "\n",
       "       d_middle  ema_10  ...  dist_to_BB  pattern_Bullish eng.  pattern_Doji  \\\n",
       "11666      -0.0   2.263  ...       1.932                     0             0   \n",
       "11668      -0.0   2.258  ...       1.924                     0             0   \n",
       "10180       0.0   4.025  ...       2.150                     0             0   \n",
       "405         0.0   4.025  ...       2.150                     0             0   \n",
       "11670      -0.0   2.222  ...       1.752                     0             1   \n",
       "\n",
       "       pattern_Hammer  pattern_Harami  pattern_no  origin_lower  origin_upper  \\\n",
       "11666               0               0           1             1             0   \n",
       "11668               0               0           1             1             0   \n",
       "10180               0               0           1             0             1   \n",
       "405                 0               0           1             0             1   \n",
       "11670               0               0           0             1             0   \n",
       "\n",
       "       candle_color_green  candle_color_red  \n",
       "11666                   0                 1  \n",
       "11668                   1                 0  \n",
       "10180                   0                 1  \n",
       "405                     0                 1  \n",
       "11670                   1                 0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For now go with the simplest One-Hot encoding provided by pandas:\n",
    "X = pd.get_dummies(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable\n",
    "y = trades_df.profitability_s15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train, validation, and test set\n",
    "\n",
    "![Split Data](img/split_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=99)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.2\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the data were split correctly\n",
    "for dataset in [y_train, y_val, y_test]:\n",
    "    print(round(len(dataset) / len(y), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to compare models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "\n",
    "![CV](img/CV.png)\n",
    "![Cross-Val](img/Cross-Val.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "![C](img/c.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  35 | elapsed:    6.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'C': 1000}\n",
      "\n",
      "0.56 (+/-0.006) for {'C': 0.001}\n",
      "0.567 (+/-0.005) for {'C': 0.01}\n",
      "0.578 (+/-0.005) for {'C': 0.1}\n",
      "0.602 (+/-0.018) for {'C': 1}\n",
      "0.617 (+/-0.02) for {'C': 10}\n",
      "0.618 (+/-0.02) for {'C': 100}\n",
      "0.618 (+/-0.023) for {'C': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Find the best model by tuning C using GridSearch CV:\n",
    "\n",
    "lr = LogisticRegression()\n",
    "parameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(lr, parameters, cv=5, verbose=5, n_jobs=-1)\n",
    "cv.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best model\n",
    "#log_reg_best = cv.best_estimator_\n",
    "\n",
    "with open('LR_best.pickle', 'wb') as f:\n",
    "    pickle.dump(log_reg_best, f)\n",
    "\n",
    "log_reg_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "![c](img/c_svm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  30 | elapsed: 31.6min remaining: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 68.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "0.562 (+/-0.005) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.56 (+/-0.0) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "0.593 (+/-0.017) for {'C': 1, 'kernel': 'linear'}\n",
      "0.571 (+/-0.019) for {'C': 1, 'kernel': 'rbf'}\n",
      "0.603 (+/-0.018) for {'C': 10, 'kernel': 'linear'}\n",
      "0.56 (+/-0.013) for {'C': 10, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(svc, parameters, cv=5, verbose=5, n_jobs=-1)\n",
    "cv.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best model\n",
    "#svm_best = cv.best_estimator_\n",
    "\n",
    "with open('SVM_best.pickle', 'wb') as f:\n",
    "    pickle.dump(svm_best, f)\n",
    "\n",
    "svm_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "\n",
    "print(MLPRegressor())\n",
    "print(MLPClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "![hidden layer](img/hidden_layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:   39.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}\n",
      "\n",
      "0.565 (+/-0.022) for {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}\n",
      "0.573 (+/-0.036) for {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}\n",
      "0.555 (+/-0.044) for {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}\n",
      "0.547 (+/-0.051) for {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "0.549 (+/-0.055) for {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}\n",
      "0.547 (+/-0.056) for {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}\n",
      "0.54 (+/-0.088) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "0.555 (+/-0.051) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}\n",
      "0.556 (+/-0.023) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "0.558 (+/-0.005) for {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}\n",
      "0.556 (+/-0.007) for {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}\n",
      "0.558 (+/-0.01) for {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}\n",
      "0.555 (+/-0.008) for {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "0.55 (+/-0.02) for {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}\n",
      "0.552 (+/-0.023) for {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}\n",
      "0.56 (+/-0.023) for {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "0.543 (+/-0.03) for {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}\n",
      "0.552 (+/-0.019) for {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "0.556 (+/-0.007) for {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}\n",
      "0.556 (+/-0.008) for {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}\n",
      "0.559 (+/-0.004) for {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}\n",
      "0.555 (+/-0.03) for {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "0.564 (+/-0.011) for {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}\n",
      "0.558 (+/-0.019) for {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}\n",
      "0.56 (+/-0.015) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "0.555 (+/-0.016) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}\n",
      "0.557 (+/-0.022) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(mlp, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "cv.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10,), learning_rate='invscaling',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mlp_best = cv.best_estimator_\n",
    "\n",
    "with open('MLP_best.pickle', 'wb') as f:\n",
    "    pickle.dump(mlp_best, f)\n",
    "\n",
    "mlp_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "![RF](img/rf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:   18.8s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'max_depth': 16, 'n_estimators': 250}\n",
      "\n",
      "0.587 (+/-0.042) for {'max_depth': 2, 'n_estimators': 5}\n",
      "0.561 (+/-0.008) for {'max_depth': 2, 'n_estimators': 50}\n",
      "0.562 (+/-0.004) for {'max_depth': 2, 'n_estimators': 250}\n",
      "0.585 (+/-0.043) for {'max_depth': 4, 'n_estimators': 5}\n",
      "0.595 (+/-0.031) for {'max_depth': 4, 'n_estimators': 50}\n",
      "0.591 (+/-0.016) for {'max_depth': 4, 'n_estimators': 250}\n",
      "0.597 (+/-0.028) for {'max_depth': 8, 'n_estimators': 5}\n",
      "0.627 (+/-0.015) for {'max_depth': 8, 'n_estimators': 50}\n",
      "0.628 (+/-0.011) for {'max_depth': 8, 'n_estimators': 250}\n",
      "0.614 (+/-0.015) for {'max_depth': 16, 'n_estimators': 5}\n",
      "0.641 (+/-0.012) for {'max_depth': 16, 'n_estimators': 50}\n",
      "0.654 (+/-0.008) for {'max_depth': 16, 'n_estimators': 250}\n",
      "0.605 (+/-0.029) for {'max_depth': 32, 'n_estimators': 5}\n",
      "0.64 (+/-0.014) for {'max_depth': 32, 'n_estimators': 50}\n",
      "0.652 (+/-0.012) for {'max_depth': 32, 'n_estimators': 250}\n",
      "0.609 (+/-0.024) for {'max_depth': None, 'n_estimators': 5}\n",
      "0.641 (+/-0.008) for {'max_depth': None, 'n_estimators': 50}\n",
      "0.651 (+/-0.003) for {'max_depth': None, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250],\n",
    "    'max_depth': [2, 4, 8, 16, 32, None]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(rf, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "cv.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=16, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=250,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rf_best = cv.best_estimator_\n",
    "\n",
    "with open('RF_best.pickle', 'wb') as f:\n",
    "    pickle.dump(rf_best, f)\n",
    "\n",
    "rf_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "![GB](img/gb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0648s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.9641s.) Setting batch_size=3.\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (38.2243s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 279 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 321 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 344 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 367 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 422 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 476 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  9.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "\n",
      "0.56 (+/-0.0) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.56 (+/-0.0) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.618 (+/-0.008) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.622 (+/-0.013) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.56 (+/-0.0) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.596 (+/-0.01) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.634 (+/-0.005) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.637 (+/-0.009) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.56 (+/-0.0) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.616 (+/-0.027) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.64 (+/-0.015) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.643 (+/-0.023) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.56 (+/-0.0) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.63 (+/-0.011) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.643 (+/-0.009) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.643 (+/-0.017) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.56 (+/-0.0) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.628 (+/-0.009) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.641 (+/-0.013) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.646 (+/-0.018) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.56 (+/-0.0) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.623 (+/-0.012) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.631 (+/-0.011) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.631 (+/-0.006) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.607 (+/-0.022) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.635 (+/-0.014) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.644 (+/-0.012) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.645 (+/-0.015) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.618 (+/-0.028) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.64 (+/-0.014) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.648 (+/-0.014) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.649 (+/-0.012) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.629 (+/-0.006) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.643 (+/-0.017) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.638 (+/-0.016) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.637 (+/-0.023) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.622 (+/-0.009) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.642 (+/-0.013) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.649 (+/-0.012) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.643 (+/-0.019) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.617 (+/-0.01) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.628 (+/-0.022) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.633 (+/-0.017) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.635 (+/-0.019) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.626 (+/-0.013) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.613 (+/-0.025) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.617 (+/-0.024) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.622 (+/-0.033) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.623 (+/-0.018) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.621 (+/-0.013) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.628 (+/-0.019) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.634 (+/-0.023) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.611 (+/-0.027) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.622 (+/-0.006) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.632 (+/-0.014) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.634 (+/-0.014) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.619 (+/-0.011) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.634 (+/-0.025) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.631 (+/-0.009) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.63 (+/-0.009) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.432 (+/-0.033) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.432 (+/-0.033) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.432 (+/-0.033) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.432 (+/-0.033) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.436 (+/-0.073) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.437 (+/-0.075) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.437 (+/-0.075) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.437 (+/-0.075) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.474 (+/-0.075) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.467 (+/-0.083) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.467 (+/-0.083) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.475 (+/-0.075) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.542 (+/-0.043) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.507 (+/-0.078) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.547 (+/-0.037) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.534 (+/-0.048) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.503 (+/-0.043) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.507 (+/-0.039) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.508 (+/-0.067) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.5 (+/-0.03) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 500}\n",
      "0.436 (+/-0.017) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 5}\n",
      "0.436 (+/-0.017) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 50}\n",
      "0.436 (+/-0.017) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 250}\n",
      "0.436 (+/-0.017) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 500}\n",
      "0.426 (+/-0.097) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 5}\n",
      "0.426 (+/-0.097) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 50}\n",
      "0.426 (+/-0.097) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 250}\n",
      "0.426 (+/-0.097) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 500}\n",
      "0.397 (+/-0.011) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 5}\n",
      "0.397 (+/-0.01) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 50}\n",
      "0.397 (+/-0.01) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 250}\n",
      "0.397 (+/-0.01) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 500}\n",
      "0.438 (+/-0.05) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 5}\n",
      "0.431 (+/-0.074) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 50}\n",
      "0.435 (+/-0.065) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 250}\n",
      "0.433 (+/-0.064) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 500}\n",
      "0.487 (+/-0.063) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 5}\n",
      "0.47 (+/-0.072) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 50}\n",
      "0.504 (+/-0.037) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 250}\n",
      "0.471 (+/-0.061) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 250, 500],\n",
    "    'max_depth': [1, 3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(gb, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "cv.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gb_best = cv.best_estimator_\n",
    "\n",
    "with open('GB_best.pickle', 'wb') as f:\n",
    "    pickle.dump(gb_best, f)\n",
    "\n",
    "gb_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Compare model results and final model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in models\n",
    "models = {}\n",
    "\n",
    "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
    "    models[mdl] = pickle.load( open(f'{mdl}_best.pickle', 'rb') ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'SVM': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "     kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "     shrinking=True, tol=0.001, verbose=False),\n",
       " 'MLP': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(10,), learning_rate='invscaling',\n",
       "               learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "               n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "               random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "               validation_fraction=0.1, verbose=False, warm_start=False),\n",
       " 'RF': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=16, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=250,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'GB': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                            n_iter_no_change=None, presort='auto',\n",
       "                            random_state=None, subsample=1.0, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=0,\n",
       "                            warm_start=False)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on the validation set\n",
    "\n",
    "![Evaluation Metrics](img/eval_metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "\n",
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.624 / Precision: 0.651 / Recall: 0.377 / Latency: 8.5ms\n",
      "SVM -- Accuracy: 0.592 / Precision: 0.599 / Recall: 0.313 / Latency: 283.2ms\n",
      "MLP -- Accuracy: 0.562 / Precision: 0.597 / Recall: 0.114 / Latency: 4.0ms\n",
      "RF -- Accuracy: 0.66 / Precision: 0.651 / Recall: 0.546 / Latency: 104.7ms\n",
      "GB -- Accuracy: 0.649 / Precision: 0.623 / Recall: 0.583 / Latency: 26.9ms\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest -- Accuracy: 0.659 / Precision: 0.611 / Recall: 0.553 / Latency: 113.2ms\n",
      "Gradient Boosting -- Accuracy: 0.667 / Precision: 0.612 / Recall: 0.599 / Latency: 24.9ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Random Forest', models['RF'], X_test, y_test)\n",
    "evaluate_model('Gradient Boosting', models['GB'], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
