{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle # to save and load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ’®¬ ў гбва®©бвўҐ D Ё¬ҐҐв ¬ҐвЄг Ќ®ўл© в®¬\n",
      " ‘ҐаЁ©­л© ­®¬Ґа в®¬ : F89F-7E89\n",
      "\n",
      " ‘®¤Ґа¦Ё¬®Ґ Ї ЇЄЁ D:\\PROJECTS\\Binance\\Trading_bot_working\\ML-training\n",
      "\n",
      "25.10.2020  15:10    <DIR>          .\n",
      "25.10.2020  15:10    <DIR>          ..\n",
      "04.10.2020  16:17    <DIR>          .ipynb_checkpoints\n",
      "12.09.2020  21:07        10я294я827 buy_signals_backtest_ML_Jan2019-Aug2020.dat\n",
      "13.09.2020  06:27         3я578я133 buy_signals_backtest_ML_July-Aug2020.dat\n",
      "25.10.2020  11:49         4я068я842 buy_signals_backtest_ML_July-Aug2020_volume.dat\n",
      "25.10.2020  15:10           151я829 C1M_ML_analysis-volume.ipynb\n",
      "04.10.2020  16:16           128я423 C1M_ML_analysis.ipynb\n",
      "16.09.2020  07:54             8я867 C1M_ML_analysis.py\n",
      "16.09.2020  07:54             1я034 logregression.pickle\n",
      "               7 д ©«®ў     18я231я955 Ў ©в\n",
      "               3 Ї Ї®Є  30я652я940я288 Ў ©в бў®Ў®¤­®\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trades_df = pd.read_csv('buy_signals_backtest_ML_July-Aug2020.dat')\n",
    "trades_df = pd.read_csv('buy_signals_backtest_ML_July-Aug2020_volume.dat')\n",
    "#trades_df.set_index('time_curr', inplace=True)\n",
    "#trades_df.index=pd.to_datetime(trades_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_curr', 'symbol', 'price', 'pattern', 'origin', 'ranging',\n",
       "       'd_ranging', 'lower', 'upper', 'middle', 'd_lower', 'd_upper',\n",
       "       'd_middle', 'ema_10', 'ema_200', 'd_ema_10', 'd_ema_200', 'k_15',\n",
       "       'd_15', 'd_k_15', 'd_d_15', 'stoch_cond_5', 'candle_color', 'volume',\n",
       "       'ntrades', 'tb_volume', 'vol_inc', 'dist_to_BB', 'profit', 'elapsed',\n",
       "       'min_price', 'max_price', 'profit_s15', 'elapsed_s15', 'min_price_s15',\n",
       "       'max_price_s15', 'Unnamed: 36'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "profitability = [0 if item < 0 else 1 for item in trades_df['profit']]\n",
    "trades_df['profitability'] = profitability\n",
    "profitability_s15 = [0 if item < 0 else 1 for item in trades_df['profit_s15']]\n",
    "trades_df['profitability_s15'] = profitability_s15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 'time_curr' to `datetime` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df['time_curr'] = pd.to_datetime(trades_df['time_curr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by the date:\n",
    "trades_df.sort_values(by=['time_curr'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_curr', 'symbol', 'price', 'pattern', 'origin', 'ranging',\n",
       "       'd_ranging', 'lower', 'upper', 'middle', 'd_lower', 'd_upper',\n",
       "       'd_middle', 'ema_10', 'ema_200', 'd_ema_10', 'd_ema_200', 'k_15',\n",
       "       'd_15', 'd_k_15', 'd_d_15', 'stoch_cond_5', 'candle_color', 'volume',\n",
       "       'ntrades', 'tb_volume', 'vol_inc', 'dist_to_BB', 'profit', 'elapsed',\n",
       "       'min_price', 'max_price', 'profit_s15', 'elapsed_s15', 'min_price_s15',\n",
       "       'max_price_s15', 'Unnamed: 36', 'profitability', 'profitability_s15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>price</th>\n",
       "      <th>pattern</th>\n",
       "      <th>origin</th>\n",
       "      <th>ranging</th>\n",
       "      <th>d_ranging</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>middle</th>\n",
       "      <th>d_lower</th>\n",
       "      <th>...</th>\n",
       "      <th>elapsed</th>\n",
       "      <th>min_price</th>\n",
       "      <th>max_price</th>\n",
       "      <th>profit_s15</th>\n",
       "      <th>elapsed_s15</th>\n",
       "      <th>min_price_s15</th>\n",
       "      <th>max_price_s15</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "      <th>profitability</th>\n",
       "      <th>profitability_s15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_curr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-09-01 00:48:00</td>\n",
       "      <td>OMGBTC</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>no</td>\n",
       "      <td>lower</td>\n",
       "      <td>5.694</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>-1.000000e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>346.0</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>309.0</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>1.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-01 07:10:00</td>\n",
       "      <td>OMGBTC</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>no</td>\n",
       "      <td>lower</td>\n",
       "      <td>4.432</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>-1.100000e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>243.0</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>243.0</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>1.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-01 07:16:00</td>\n",
       "      <td>OMGBTC</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>no</td>\n",
       "      <td>lower</td>\n",
       "      <td>4.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>-8.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>237.0</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>1.47</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>237.0</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>1.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-01 16:05:00</td>\n",
       "      <td>LRCBTC</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>no</td>\n",
       "      <td>lower</td>\n",
       "      <td>5.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-1.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-01 16:15:00</td>\n",
       "      <td>QTUMBTC</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>Harami</td>\n",
       "      <td>lower</td>\n",
       "      <td>4.533</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>-5.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>1.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-30 19:15:00</td>\n",
       "      <td>FETBTC</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>no</td>\n",
       "      <td>lower</td>\n",
       "      <td>6.759</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.5</td>\n",
       "      <td>109.0</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-30 19:16:00</td>\n",
       "      <td>SXPBTC</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>Harami</td>\n",
       "      <td>lower</td>\n",
       "      <td>4.184</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>-2.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.5</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-30 19:17:00</td>\n",
       "      <td>RENBTC</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>Hammer</td>\n",
       "      <td>lower</td>\n",
       "      <td>5.630</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>1.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-30 20:15:00</td>\n",
       "      <td>FTMBTC</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>Doji</td>\n",
       "      <td>upper</td>\n",
       "      <td>4.149</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>205.0</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-30 20:46:00</td>\n",
       "      <td>DIABTC</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>no</td>\n",
       "      <td>upper</td>\n",
       "      <td>7.843</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>1.400000e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4244 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      symbol     price pattern origin  ranging  d_ranging  \\\n",
       "time_curr                                                                   \n",
       "2020-09-01 00:48:00   OMGBTC  0.000437      no  lower    5.694     -0.050   \n",
       "2020-09-01 07:10:00   OMGBTC  0.000428      no  lower    4.432      0.020   \n",
       "2020-09-01 07:16:00   OMGBTC  0.000428      no  lower    4.433      0.000   \n",
       "2020-09-01 16:05:00   LRCBTC  0.000018      no  lower    5.087      0.000   \n",
       "2020-09-01 16:15:00  QTUMBTC  0.000287  Harami  lower    4.533      0.012   \n",
       "...                      ...       ...     ...    ...      ...        ...   \n",
       "2020-09-30 19:15:00   FETBTC  0.000006      no  lower    6.759      0.013   \n",
       "2020-09-30 19:16:00   SXPBTC  0.000155  Harami  lower    4.184      0.005   \n",
       "2020-09-30 19:17:00   RENBTC  0.000022  Hammer  lower    5.630      0.025   \n",
       "2020-09-30 20:15:00   FTMBTC  0.000003    Doji  upper    4.149     -0.002   \n",
       "2020-09-30 20:46:00   DIABTC  0.000129      no  upper    7.843     -0.101   \n",
       "\n",
       "                        lower     upper    middle       d_lower  ...  elapsed  \\\n",
       "time_curr                                                        ...            \n",
       "2020-09-01 00:48:00  0.000431  0.000459  0.000445 -1.000000e-07  ...    346.0   \n",
       "2020-09-01 07:10:00  0.000426  0.000444  0.000435 -1.100000e-07  ...    243.0   \n",
       "2020-09-01 07:16:00  0.000425  0.000444  0.000435 -8.000000e-08  ...    237.0   \n",
       "2020-09-01 16:05:00  0.000018  0.000019  0.000018 -1.000000e-08  ...     35.0   \n",
       "2020-09-01 16:15:00  0.000287  0.000299  0.000293 -5.000000e-08  ...     24.0   \n",
       "...                       ...       ...       ...           ...  ...      ...   \n",
       "2020-09-30 19:15:00  0.000006  0.000007  0.000007 -0.000000e+00  ...    109.0   \n",
       "2020-09-30 19:16:00  0.000155  0.000161  0.000158 -2.000000e-08  ...     56.0   \n",
       "2020-09-30 19:17:00  0.000022  0.000023  0.000022 -0.000000e+00  ...     19.0   \n",
       "2020-09-30 20:15:00  0.000003  0.000004  0.000003  0.000000e+00  ...    205.0   \n",
       "2020-09-30 20:46:00  0.000124  0.000135  0.000129  1.400000e-07  ...     31.0   \n",
       "\n",
       "                     min_price  max_price  profit_s15  elapsed_s15  \\\n",
       "time_curr                                                            \n",
       "2020-09-01 00:48:00      -2.50       1.21        -1.5        309.0   \n",
       "2020-09-01 07:10:00      -2.50       1.40        -1.5        243.0   \n",
       "2020-09-01 07:16:00      -2.50       1.47        -1.5        237.0   \n",
       "2020-09-01 16:05:00       0.00       1.29         1.5         35.0   \n",
       "2020-09-01 16:15:00      -0.24       1.77         1.5         24.0   \n",
       "...                        ...        ...         ...          ...   \n",
       "2020-09-30 19:15:00      -0.31       1.26         1.5        109.0   \n",
       "2020-09-30 19:16:00      -0.94       1.60         1.5         56.0   \n",
       "2020-09-30 19:17:00       0.00       0.00         1.5         19.0   \n",
       "2020-09-30 20:15:00      -2.29       2.00        -1.5         64.0   \n",
       "2020-09-30 20:46:00      -2.50       0.23        -1.5         29.0   \n",
       "\n",
       "                     min_price_s15  max_price_s15  Unnamed: 36  profitability  \\\n",
       "time_curr                                                                       \n",
       "2020-09-01 00:48:00          -1.25           1.21          NaN              0   \n",
       "2020-09-01 07:10:00          -1.25           1.40          NaN              0   \n",
       "2020-09-01 07:16:00          -1.25           1.47          NaN              0   \n",
       "2020-09-01 16:05:00           0.00           1.29          NaN              1   \n",
       "2020-09-01 16:15:00          -0.24           1.77          NaN              1   \n",
       "...                            ...            ...          ...            ...   \n",
       "2020-09-30 19:15:00          -0.31           1.26          NaN              1   \n",
       "2020-09-30 19:16:00          -0.94           1.60          NaN              1   \n",
       "2020-09-30 19:17:00          -0.19           1.76          NaN              1   \n",
       "2020-09-30 20:15:00          -1.25           0.29          NaN              1   \n",
       "2020-09-30 20:46:00          -1.25           0.23          NaN              0   \n",
       "\n",
       "                     profitability_s15  \n",
       "time_curr                               \n",
       "2020-09-01 00:48:00                  0  \n",
       "2020-09-01 07:10:00                  0  \n",
       "2020-09-01 07:16:00                  0  \n",
       "2020-09-01 16:05:00                  1  \n",
       "2020-09-01 16:15:00                  1  \n",
       "...                                ...  \n",
       "2020-09-30 19:15:00                  1  \n",
       "2020-09-30 19:16:00                  1  \n",
       "2020-09-30 19:17:00                  1  \n",
       "2020-09-30 20:15:00                  0  \n",
       "2020-09-30 20:46:00                  0  \n",
       "\n",
       "[4244 rows x 38 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df.set_index('time_curr')[\"2020-09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-10-25 11:03:00')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df.time_curr.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trade: 2020-07-02 00:00:00, end trade: 2020-10-25 11:03:00 \n",
      "Total profitability in 115 days 11:03:00:  0.6163383874167493\n",
      "total n trades:  14114\n",
      "Total profitability in 115 days 11:03:00 with 0.15 stop loss:  0.4427518775683718\n",
      "total n trades:  14114\n"
     ]
    }
   ],
   "source": [
    "time_span = trades_df.time_curr.iloc[-1] - trades_df.time_curr.iloc[0]\n",
    "print(f\"Start trade: {trades_df.time_curr.iloc[0]}, end trade: {trades_df.time_curr.iloc[-1]} \")\n",
    "print(f\"Total profitability in {time_span}: \", trades_df['profitability'].sum()/len(trades_df['profitability']) )\n",
    "print(\"total n trades: \", len(trades_df['profitability']))\n",
    "print(f\"Total profitability in {time_span} with 0.15 stop loss: \", trades_df['profitability_s15'].sum()/len(trades_df['profitability_s15']) )\n",
    "print(\"total n trades: \", len(trades_df['profitability_s15']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.drop(['Unnamed: 36'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12643    1\n",
       "12644    1\n",
       "12645    1\n",
       "11653    0\n",
       "11654    0\n",
       "        ..\n",
       "13351    1\n",
       "14084    1\n",
       "13837    1\n",
       "13792    0\n",
       "14113    1\n",
       "Name: profitability, Length: 14114, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df['profitability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trades_df.dropna(axis=1, inplace=True)\n",
    "trades_df.fillna(-9999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df = trades_df[trades_df.d_ema_200 != -9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the problematic coin that has been identified later\n",
    "#trades_df = trades_df[trades_df.symbol != 'UMABTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>ranging</th>\n",
       "      <th>d_ranging</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>middle</th>\n",
       "      <th>d_lower</th>\n",
       "      <th>d_upper</th>\n",
       "      <th>d_middle</th>\n",
       "      <th>ema_10</th>\n",
       "      <th>ema_200</th>\n",
       "      <th>d_ema_10</th>\n",
       "      <th>d_ema_200</th>\n",
       "      <th>k_15</th>\n",
       "      <th>d_15</th>\n",
       "      <th>d_k_15</th>\n",
       "      <th>d_d_15</th>\n",
       "      <th>volume</th>\n",
       "      <th>ntrades</th>\n",
       "      <th>tb_volume</th>\n",
       "      <th>vol_inc</th>\n",
       "      <th>dist_to_BB</th>\n",
       "      <th>profit</th>\n",
       "      <th>elapsed</th>\n",
       "      <th>min_price</th>\n",
       "      <th>max_price</th>\n",
       "      <th>profit_s15</th>\n",
       "      <th>elapsed_s15</th>\n",
       "      <th>min_price_s15</th>\n",
       "      <th>max_price_s15</th>\n",
       "      <th>profitability</th>\n",
       "      <th>profitability_s15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>1.356200e+04</td>\n",
       "      <td>1.356200e+04</td>\n",
       "      <td>1.356200e+04</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>1.356200e+04</td>\n",
       "      <td>1.356200e+04</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "      <td>13562.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>8.736665</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-1.101106e-07</td>\n",
       "      <td>4.999263e-10</td>\n",
       "      <td>-5.459740e-08</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>-9.477511e-08</td>\n",
       "      <td>-9.643120e-09</td>\n",
       "      <td>17.985580</td>\n",
       "      <td>11.538269</td>\n",
       "      <td>0.511211</td>\n",
       "      <td>0.265893</td>\n",
       "      <td>12.384139</td>\n",
       "      <td>483.760581</td>\n",
       "      <td>5.738814</td>\n",
       "      <td>1.291381</td>\n",
       "      <td>3.362969</td>\n",
       "      <td>-0.235695</td>\n",
       "      <td>116.094971</td>\n",
       "      <td>-1.426470</td>\n",
       "      <td>1.423317</td>\n",
       "      <td>-0.179177</td>\n",
       "      <td>60.396697</td>\n",
       "      <td>-0.887215</td>\n",
       "      <td>1.153465</td>\n",
       "      <td>0.614290</td>\n",
       "      <td>0.440274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>5.157826</td>\n",
       "      <td>0.048464</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>4.455224e-07</td>\n",
       "      <td>2.236064e-07</td>\n",
       "      <td>2.107911e-07</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>3.198659e-07</td>\n",
       "      <td>7.064269e-08</td>\n",
       "      <td>6.331302</td>\n",
       "      <td>5.631111</td>\n",
       "      <td>0.269665</td>\n",
       "      <td>0.163142</td>\n",
       "      <td>30.131103</td>\n",
       "      <td>731.098935</td>\n",
       "      <td>13.987773</td>\n",
       "      <td>2.993101</td>\n",
       "      <td>2.113424</td>\n",
       "      <td>2.190512</td>\n",
       "      <td>224.325547</td>\n",
       "      <td>1.023047</td>\n",
       "      <td>1.295999</td>\n",
       "      <td>1.489315</td>\n",
       "      <td>116.914853</td>\n",
       "      <td>0.484092</td>\n",
       "      <td>1.146830</td>\n",
       "      <td>0.486781</td>\n",
       "      <td>0.496438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.515000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-9.980000e-06</td>\n",
       "      <td>-1.008000e-05</td>\n",
       "      <td>-4.110000e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-6.850000e-06</td>\n",
       "      <td>-2.170000e-06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-5.020000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-5.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>5.421000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-6.000000e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.000000e-08</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-5.000000e-08</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>12.110000</td>\n",
       "      <td>6.940000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.887000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.797400</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>2.001000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>7.250500</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-1.000000e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-1.000000e-08</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>16.750000</td>\n",
       "      <td>10.550000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>4.295500</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.907893</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>2.722500</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>-1.505000</td>\n",
       "      <td>1.510000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>10.327750</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>23.357500</td>\n",
       "      <td>15.350000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>10.649750</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>4.820838</td>\n",
       "      <td>1.384000</td>\n",
       "      <td>4.015000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>1.920000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>86.989000</td>\n",
       "      <td>1.274000</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.008673</td>\n",
       "      <td>4.440000e-06</td>\n",
       "      <td>5.350000e-06</td>\n",
       "      <td>1.560000e-06</td>\n",
       "      <td>0.008670</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>4.900000e-07</td>\n",
       "      <td>9.800000e-07</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.900000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>942.345000</td>\n",
       "      <td>18510.000000</td>\n",
       "      <td>369.195413</td>\n",
       "      <td>175.207000</td>\n",
       "      <td>35.896000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>5916.000000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>36.020000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3498.000000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>33.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price       ranging     d_ranging         lower         upper  \\\n",
       "count  13562.000000  13562.000000  13562.000000  13562.000000  13562.000000   \n",
       "mean       0.000374      8.736665      0.018527      0.000370      0.000400   \n",
       "std        0.000962      5.157826      0.048464      0.000952      0.001028   \n",
       "min        0.000002      4.000000     -0.515000      0.000002      0.000002   \n",
       "25%        0.000011      5.421000      0.001000      0.000011      0.000011   \n",
       "50%        0.000050      7.250500      0.016000      0.000049      0.000054   \n",
       "75%        0.000257     10.327750      0.034000      0.000254      0.000278   \n",
       "max        0.008671     86.989000      1.274000      0.008469      0.008904   \n",
       "\n",
       "             middle       d_lower       d_upper      d_middle        ema_10  \\\n",
       "count  13562.000000  1.356200e+04  1.356200e+04  1.356200e+04  13562.000000   \n",
       "mean       0.000385 -1.101106e-07  4.999263e-10 -5.459740e-08      0.000379   \n",
       "std        0.000990  4.455224e-07  2.236064e-07  2.107911e-07      0.000975   \n",
       "min        0.000002 -9.980000e-06 -1.008000e-05 -4.110000e-06      0.000002   \n",
       "25%        0.000011 -6.000000e-08  0.000000e+00 -3.000000e-08      0.000011   \n",
       "50%        0.000051 -1.000000e-08  0.000000e+00 -0.000000e+00      0.000051   \n",
       "75%        0.000266 -0.000000e+00  0.000000e+00  0.000000e+00      0.000261   \n",
       "max        0.008673  4.440000e-06  5.350000e-06  1.560000e-06      0.008670   \n",
       "\n",
       "            ema_200      d_ema_10     d_ema_200          k_15          d_15  \\\n",
       "count  13562.000000  1.356200e+04  1.356200e+04  13562.000000  13562.000000   \n",
       "mean       0.000387 -9.477511e-08 -9.643120e-09     17.985580     11.538269   \n",
       "std        0.001000  3.198659e-07  7.064269e-08      6.331302      5.631111   \n",
       "min        0.000002 -6.850000e-06 -2.170000e-06     10.000000      3.330000   \n",
       "25%        0.000011 -5.000000e-08 -0.000000e+00     12.110000      6.940000   \n",
       "50%        0.000051 -1.000000e-08 -0.000000e+00     16.750000     10.550000   \n",
       "75%        0.000271 -0.000000e+00  0.000000e+00     23.357500     15.350000   \n",
       "max        0.008404  4.900000e-07  9.800000e-07     30.000000     29.900000   \n",
       "\n",
       "             d_k_15        d_d_15        volume       ntrades     tb_volume  \\\n",
       "count  13562.000000  13562.000000  13562.000000  13562.000000  13562.000000   \n",
       "mean       0.511211      0.265893     12.384139    483.760581      5.738814   \n",
       "std        0.269665      0.163142     30.131103    731.098935     13.987773   \n",
       "min        0.000000      0.000000      0.007000      2.000000      0.000400   \n",
       "25%        0.320000      0.150000      1.887000    140.000000      0.797400   \n",
       "50%        0.500000      0.240000      4.295500    266.000000      1.907893   \n",
       "75%        0.690000      0.370000     10.649750    538.000000      4.820838   \n",
       "max        2.000000      0.670000    942.345000  18510.000000    369.195413   \n",
       "\n",
       "            vol_inc    dist_to_BB        profit       elapsed     min_price  \\\n",
       "count  13562.000000  13562.000000  13562.000000  13562.000000  13562.000000   \n",
       "mean       1.291381      3.362969     -0.235695    116.094971     -1.426470   \n",
       "std        2.993101      2.113424      2.190512    224.325547      1.023047   \n",
       "min        0.000000      1.500000     -3.000000      1.000000     -2.500000   \n",
       "25%        0.271000      2.001000     -3.000000     20.000000     -2.500000   \n",
       "50%        0.678000      2.722500      1.500000     51.000000     -1.505000   \n",
       "75%        1.384000      4.015000      1.500000    126.000000     -0.380000   \n",
       "max      175.207000     35.896000      1.500000   5916.000000      0.830000   \n",
       "\n",
       "          max_price    profit_s15   elapsed_s15  min_price_s15  max_price_s15  \\\n",
       "count  13562.000000  13562.000000  13562.000000   13562.000000   13562.000000   \n",
       "mean       1.423317     -0.179177     60.396697      -0.887215       1.153465   \n",
       "std        1.295999      1.489315    116.914853       0.484092       1.146830   \n",
       "min       -5.020000     -1.500000      1.000000      -1.250000      -5.020000   \n",
       "25%        0.470000     -1.500000     11.000000      -1.250000       0.270000   \n",
       "50%        1.510000     -1.500000     26.500000      -1.250000       0.970000   \n",
       "75%        1.920000      1.500000     66.000000      -0.450000       1.720000   \n",
       "max       36.020000      1.500000   3498.000000       0.830000      33.920000   \n",
       "\n",
       "       profitability  profitability_s15  \n",
       "count   13562.000000       13562.000000  \n",
       "mean        0.614290           0.440274  \n",
       "std         0.486781           0.496438  \n",
       "min         0.000000           0.000000  \n",
       "25%         0.000000           0.000000  \n",
       "50%         1.000000           0.000000  \n",
       "75%         1.000000           1.000000  \n",
       "max         1.000000           1.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we start preparing the dataset to train a model using various Machine Learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trades_df.drop(['symbol','profit',\n",
    "                    'stoch_cond_5','vol_inc',\n",
    "       'elapsed', 'min_price', 'max_price', 'profit_s15', 'elapsed_s15',\n",
    "       'min_price_s15', 'max_price_s15',  'profitability',\n",
    "       'profitability_s15'], axis=1)\n",
    "\n",
    "# For the 1st trial, let us also drop all categorical variables:\n",
    "# X = trades_df.drop(['time_curr','profit',\n",
    "#                     'pattern','origin','symbol','stoch_cond_5','candle_color',\n",
    "#        'elapsed', 'min_price', 'max_price', 'profit_s15', 'elapsed_s15',\n",
    "#        'min_price_s15', 'max_price_s15',  'profitability',\n",
    "#        'profitability_s15'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['stoch_cond_5'] = X['stoch_cond_5'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Looks like features from timestamps don't help\n",
    "# X['day'] = X.time_curr.dt.day.astype('uint8')\n",
    "# X['hour'] = X.time_curr.dt.hour.astype('uint8')\n",
    "# X['minute'] = X.time_curr.dt.minute.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['time_curr'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'pattern', 'origin', 'ranging', 'd_ranging', 'lower', 'upper',\n",
       "       'middle', 'd_lower', 'd_upper', 'd_middle', 'ema_10', 'ema_200',\n",
       "       'd_ema_10', 'd_ema_200', 'k_15', 'd_15', 'd_k_15', 'd_d_15',\n",
       "       'candle_color', 'volume', 'ntrades', 'tb_volume', 'dist_to_BB'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to drop all prices and keep only the slopes and percentages: (No!)\n",
    "## X = X.drop(['price', 'lower', 'upper', 'middle', 'ema_10'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in ['d_lower', 'd_upper', 'd_middle', 'd_ema_10', 'd_ema_200' ]: X[col] = X[col]*100000\n",
    "# Now try to scale the price to 1 satoshi:\n",
    "for col in ['price','lower','upper','middle','d_lower', 'd_upper', 'd_middle',\n",
    "            'ema_10', 'ema_200','d_ema_10', 'd_ema_200' ]: X[col] = X[col]*1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>pattern</th>\n",
       "      <th>origin</th>\n",
       "      <th>ranging</th>\n",
       "      <th>d_ranging</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>middle</th>\n",
       "      <th>d_lower</th>\n",
       "      <th>d_upper</th>\n",
       "      <th>d_middle</th>\n",
       "      <th>ema_10</th>\n",
       "      <th>ema_200</th>\n",
       "      <th>d_ema_10</th>\n",
       "      <th>d_ema_200</th>\n",
       "      <th>k_15</th>\n",
       "      <th>d_15</th>\n",
       "      <th>d_k_15</th>\n",
       "      <th>d_d_15</th>\n",
       "      <th>candle_color</th>\n",
       "      <th>volume</th>\n",
       "      <th>ntrades</th>\n",
       "      <th>tb_volume</th>\n",
       "      <th>dist_to_BB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11666</td>\n",
       "      <td>2.241</td>\n",
       "      <td>no</td>\n",
       "      <td>lower</td>\n",
       "      <td>5.444</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.343</td>\n",
       "      <td>2.284</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.263</td>\n",
       "      <td>2.403</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>20.38</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>red</td>\n",
       "      <td>3.301</td>\n",
       "      <td>448</td>\n",
       "      <td>1.313038</td>\n",
       "      <td>1.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11668</td>\n",
       "      <td>2.237</td>\n",
       "      <td>no</td>\n",
       "      <td>lower</td>\n",
       "      <td>5.599</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2.220</td>\n",
       "      <td>2.340</td>\n",
       "      <td>2.280</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.258</td>\n",
       "      <td>2.401</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>18.06</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>green</td>\n",
       "      <td>3.713</td>\n",
       "      <td>351</td>\n",
       "      <td>1.619265</td>\n",
       "      <td>1.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10180</td>\n",
       "      <td>4.023</td>\n",
       "      <td>no</td>\n",
       "      <td>upper</td>\n",
       "      <td>4.460</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>3.926</td>\n",
       "      <td>4.110</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.025</td>\n",
       "      <td>3.799</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.73</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>red</td>\n",
       "      <td>6.100</td>\n",
       "      <td>202</td>\n",
       "      <td>1.633603</td>\n",
       "      <td>2.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>4.023</td>\n",
       "      <td>no</td>\n",
       "      <td>upper</td>\n",
       "      <td>4.460</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>3.926</td>\n",
       "      <td>4.110</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.025</td>\n",
       "      <td>3.799</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.73</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>red</td>\n",
       "      <td>6.100</td>\n",
       "      <td>202</td>\n",
       "      <td>1.633603</td>\n",
       "      <td>2.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11670</td>\n",
       "      <td>2.209</td>\n",
       "      <td>Doji</td>\n",
       "      <td>lower</td>\n",
       "      <td>5.350</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.191</td>\n",
       "      <td>2.304</td>\n",
       "      <td>2.248</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.222</td>\n",
       "      <td>2.390</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>11.53</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.21</td>\n",
       "      <td>green</td>\n",
       "      <td>1.096</td>\n",
       "      <td>78</td>\n",
       "      <td>0.508115</td>\n",
       "      <td>1.752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       price pattern origin  ranging  d_ranging  lower  upper  middle  \\\n",
       "11666  2.241      no  lower    5.444      0.011  2.226  2.343   2.284   \n",
       "11668  2.237      no  lower    5.599      0.010  2.220  2.340   2.280   \n",
       "10180  4.023      no  upper    4.460     -0.014  3.926  4.110   4.018   \n",
       "405    4.023      no  upper    4.460     -0.014  3.926  4.110   4.018   \n",
       "11670  2.209    Doji  lower    5.350      0.011  2.191  2.304   2.248   \n",
       "\n",
       "       d_lower  d_upper  d_middle  ema_10  ema_200  d_ema_10  d_ema_200  \\\n",
       "11666   -0.001     -0.0      -0.0   2.263    2.403    -0.001       -0.0   \n",
       "11668   -0.000     -0.0      -0.0   2.258    2.401    -0.000       -0.0   \n",
       "10180    0.001     -0.0       0.0   4.025    3.799    -0.000        0.0   \n",
       "405      0.001     -0.0       0.0   4.025    3.799    -0.000        0.0   \n",
       "11670   -0.000     -0.0      -0.0   2.222    2.390    -0.000       -0.0   \n",
       "\n",
       "        k_15   d_15  d_k_15  d_d_15 candle_color  volume  ntrades  tb_volume  \\\n",
       "11666  20.38  17.00    0.50    0.02          red   3.301      448   1.313038   \n",
       "11668  18.06  14.85    0.30    0.01        green   3.713      351   1.619265   \n",
       "10180  18.73  15.32    0.22    0.31          red   6.100      202   1.633603   \n",
       "405    18.73  15.32    0.22    0.31          red   6.100      202   1.633603   \n",
       "11670  11.53   4.81    0.64    0.21        green   1.096       78   0.508115   \n",
       "\n",
       "       dist_to_BB  \n",
       "11666       1.932  \n",
       "11668       1.924  \n",
       "10180       2.150  \n",
       "405         2.150  \n",
       "11670       1.752  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Target:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we predict whether the trade is going to be profitable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = trades_df.profitability_s15\n",
    "# what if we predict profitability for stop loss 0.3?\n",
    "#y = trades_df.profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11666    0\n",
       "11668    0\n",
       "10180    0\n",
       "405      0\n",
       "11670    0\n",
       "        ..\n",
       "13351    0\n",
       "14084    1\n",
       "13837    1\n",
       "13792    0\n",
       "14113    1\n",
       "Name: profitability_s15, Length: 13562, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_size = 2000\n",
    "\n",
    "# Make sure that there are no new symbols in the test dataset\n",
    "\n",
    "# for item_val in X.symbol.tail(test_size).unique():\n",
    "#     if item_val not in X.symbol.iloc[:-test_size].unique(): print(item_val)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[X.symbol != 'UMABTC']\n",
    "# y = y[X.symbol != 'UMABTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for item_val in X.symbol.tail(test_size).unique():\n",
    "#     if item_val not in X.symbol.iloc[:-test_size].unique(): print(item_val)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=101)\n",
    "\n",
    "# More realistic split is where we choose the latest deals as test data. Remove last rows:\n",
    "#test_size = 2000\n",
    "test_size = 1000\n",
    "# Remove last rows from the dataset\n",
    "X_train, y_train = X.iloc[:-test_size], y.iloc[:-test_size]\n",
    "X_val, y_val = X.tail(test_size), y.tail(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now it's time to make cathegorical encodings (or wait, let's do it later!) :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical = ['pattern', 'origin', 'stoch_cond_5', 'candle_color']\n",
    "# label = ['symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in label:\n",
    "#     X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "#     X_val[col] = label_encoder.transform(X_val[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now go with the simplest One-Hot encoding provided by pandas:\n",
    "X_train, X_val = pd.get_dummies(X_train), pd.get_dummies(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_val = preprocessing.scale(X_train), preprocessing.scale(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>ranging</th>\n",
       "      <th>d_ranging</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>middle</th>\n",
       "      <th>d_lower</th>\n",
       "      <th>d_upper</th>\n",
       "      <th>d_middle</th>\n",
       "      <th>ema_10</th>\n",
       "      <th>ema_200</th>\n",
       "      <th>d_ema_10</th>\n",
       "      <th>d_ema_200</th>\n",
       "      <th>k_15</th>\n",
       "      <th>d_15</th>\n",
       "      <th>d_k_15</th>\n",
       "      <th>d_d_15</th>\n",
       "      <th>volume</th>\n",
       "      <th>ntrades</th>\n",
       "      <th>tb_volume</th>\n",
       "      <th>dist_to_BB</th>\n",
       "      <th>pattern_Bullish eng.</th>\n",
       "      <th>pattern_Doji</th>\n",
       "      <th>pattern_Hammer</th>\n",
       "      <th>pattern_Harami</th>\n",
       "      <th>pattern_no</th>\n",
       "      <th>origin_lower</th>\n",
       "      <th>origin_upper</th>\n",
       "      <th>candle_color_green</th>\n",
       "      <th>candle_color_red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11666</td>\n",
       "      <td>2.241</td>\n",
       "      <td>5.444</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.343</td>\n",
       "      <td>2.284</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>2.263</td>\n",
       "      <td>2.403</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>20.38</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.301</td>\n",
       "      <td>448</td>\n",
       "      <td>1.313038</td>\n",
       "      <td>1.932</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11668</td>\n",
       "      <td>2.237</td>\n",
       "      <td>5.599</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2.220</td>\n",
       "      <td>2.340</td>\n",
       "      <td>2.280</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>2.258</td>\n",
       "      <td>2.401</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>18.06</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.713</td>\n",
       "      <td>351</td>\n",
       "      <td>1.619265</td>\n",
       "      <td>1.924</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10180</td>\n",
       "      <td>4.023</td>\n",
       "      <td>4.460</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>3.926</td>\n",
       "      <td>4.110</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.025</td>\n",
       "      <td>3.799</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.73</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>6.100</td>\n",
       "      <td>202</td>\n",
       "      <td>1.633603</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>4.023</td>\n",
       "      <td>4.460</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>3.926</td>\n",
       "      <td>4.110</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.025</td>\n",
       "      <td>3.799</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.73</td>\n",
       "      <td>15.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>6.100</td>\n",
       "      <td>202</td>\n",
       "      <td>1.633603</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11670</td>\n",
       "      <td>2.209</td>\n",
       "      <td>5.350</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.191</td>\n",
       "      <td>2.304</td>\n",
       "      <td>2.248</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>2.222</td>\n",
       "      <td>2.390</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>11.53</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.096</td>\n",
       "      <td>78</td>\n",
       "      <td>0.508115</td>\n",
       "      <td>1.752</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>11.290</td>\n",
       "      <td>7.350</td>\n",
       "      <td>0.023</td>\n",
       "      <td>11.246</td>\n",
       "      <td>12.033</td>\n",
       "      <td>11.640</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>11.460</td>\n",
       "      <td>11.884</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>29.76</td>\n",
       "      <td>20.15</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3.171</td>\n",
       "      <td>157</td>\n",
       "      <td>1.676242</td>\n",
       "      <td>3.096</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>2.109</td>\n",
       "      <td>5.842</td>\n",
       "      <td>0.015</td>\n",
       "      <td>2.094</td>\n",
       "      <td>2.212</td>\n",
       "      <td>2.153</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>2.127</td>\n",
       "      <td>2.192</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>10.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.559</td>\n",
       "      <td>166</td>\n",
       "      <td>1.601865</td>\n",
       "      <td>2.082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>5.127</td>\n",
       "      <td>10.096</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5.054</td>\n",
       "      <td>5.560</td>\n",
       "      <td>5.307</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>5.190</td>\n",
       "      <td>5.144</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>10.04</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.989</td>\n",
       "      <td>228</td>\n",
       "      <td>2.098160</td>\n",
       "      <td>3.513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>20.029</td>\n",
       "      <td>13.075</td>\n",
       "      <td>0.040</td>\n",
       "      <td>19.659</td>\n",
       "      <td>22.112</td>\n",
       "      <td>20.886</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>20.344</td>\n",
       "      <td>21.145</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>10.65</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.478</td>\n",
       "      <td>111</td>\n",
       "      <td>0.883157</td>\n",
       "      <td>4.277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>21.700</td>\n",
       "      <td>5.782</td>\n",
       "      <td>0.026</td>\n",
       "      <td>21.609</td>\n",
       "      <td>22.774</td>\n",
       "      <td>22.191</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>21.926</td>\n",
       "      <td>22.445</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>13.58</td>\n",
       "      <td>9.92</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.833</td>\n",
       "      <td>32</td>\n",
       "      <td>0.480412</td>\n",
       "      <td>2.265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12562 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price  ranging  d_ranging   lower   upper  middle  d_lower  d_upper  \\\n",
       "11666   2.241    5.444      0.011   2.226   2.343   2.284   -0.001   -0.000   \n",
       "11668   2.237    5.599      0.010   2.220   2.340   2.280   -0.000   -0.000   \n",
       "10180   4.023    4.460     -0.014   3.926   4.110   4.018    0.001   -0.000   \n",
       "405     4.023    4.460     -0.014   3.926   4.110   4.018    0.001   -0.000   \n",
       "11670   2.209    5.350      0.011   2.191   2.304   2.248   -0.000   -0.000   \n",
       "...       ...      ...        ...     ...     ...     ...      ...      ...   \n",
       "4790   11.290    7.350      0.023  11.246  12.033  11.640   -0.003    0.000   \n",
       "1625    2.109    5.842      0.015   2.094   2.212   2.153   -0.001    0.000   \n",
       "1925    5.127   10.096      0.005   5.054   5.560   5.307   -0.002   -0.001   \n",
       "3130   20.029   13.075      0.040  19.659  22.112  20.886   -0.011    0.003   \n",
       "1218   21.700    5.782      0.026  21.609  22.774  22.191   -0.005    0.000   \n",
       "\n",
       "       d_middle  ema_10  ema_200  d_ema_10  d_ema_200   k_15   d_15  d_k_15  \\\n",
       "11666    -0.000   2.263    2.403    -0.001     -0.000  20.38  17.00    0.50   \n",
       "11668    -0.000   2.258    2.401    -0.000     -0.000  18.06  14.85    0.30   \n",
       "10180     0.000   4.025    3.799    -0.000      0.000  18.73  15.32    0.22   \n",
       "405       0.000   4.025    3.799    -0.000      0.000  18.73  15.32    0.22   \n",
       "11670    -0.000   2.222    2.390    -0.000     -0.000  11.53   4.81    0.64   \n",
       "...         ...     ...      ...       ...        ...    ...    ...     ...   \n",
       "4790     -0.002  11.460   11.884    -0.002     -0.000  29.76  20.15    0.63   \n",
       "1625     -0.000   2.127    2.192    -0.000     -0.000  10.99   5.99    0.43   \n",
       "1925     -0.001   5.190    5.144    -0.002     -0.000  10.04   4.35    0.54   \n",
       "3130     -0.004  20.344   21.145    -0.008     -0.001  10.65   5.16    0.51   \n",
       "1218     -0.002  21.926   22.445    -0.003     -0.000  13.58   9.92    0.17   \n",
       "\n",
       "       d_d_15  volume  ntrades  tb_volume  dist_to_BB  pattern_Bullish eng.  \\\n",
       "11666    0.02   3.301      448   1.313038       1.932                     0   \n",
       "11668    0.01   3.713      351   1.619265       1.924                     0   \n",
       "10180    0.31   6.100      202   1.633603       2.150                     0   \n",
       "405      0.31   6.100      202   1.633603       2.150                     0   \n",
       "11670    0.21   1.096       78   0.508115       1.752                     0   \n",
       "...       ...     ...      ...        ...         ...                   ...   \n",
       "4790     0.66   3.171      157   1.676242       3.096                     0   \n",
       "1625     0.24   5.559      166   1.601865       2.082                     0   \n",
       "1925     0.22   5.989      228   2.098160       3.513                     0   \n",
       "3130     0.24   1.478      111   0.883157       4.277                     0   \n",
       "1218     0.24   0.833       32   0.480412       2.265                     1   \n",
       "\n",
       "       pattern_Doji  pattern_Hammer  pattern_Harami  pattern_no  origin_lower  \\\n",
       "11666             0               0               0           1             1   \n",
       "11668             0               0               0           1             1   \n",
       "10180             0               0               0           1             0   \n",
       "405               0               0               0           1             0   \n",
       "11670             1               0               0           0             1   \n",
       "...             ...             ...             ...         ...           ...   \n",
       "4790              1               0               0           0             1   \n",
       "1625              0               1               0           0             1   \n",
       "1925              0               0               0           1             1   \n",
       "3130              0               0               0           1             1   \n",
       "1218              0               0               0           0             1   \n",
       "\n",
       "       origin_upper  candle_color_green  candle_color_red  \n",
       "11666             0                   0                 1  \n",
       "11668             0                   1                 0  \n",
       "10180             1                   0                 1  \n",
       "405               1                   0                 1  \n",
       "11670             0                   1                 0  \n",
       "...             ...                 ...               ...  \n",
       "4790              0                   1                 0  \n",
       "1625              0                   0                 1  \n",
       "1925              0                   0                 1  \n",
       "3130              0                   0                 1  \n",
       "1218              0                   1                 0  \n",
       "\n",
       "[12562 rows x 30 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to compare different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for comparing different approaches\n",
    "def score_dataset(model, X_train, X_val, y_train, y_val, **kwargs):\n",
    "    '''Trains a model, makes predictions. \n",
    "    Prints classification report\n",
    "    Returns mean absolute error'''\n",
    "    #Modified from: https://www.kaggle.com/alexisbcook/exercise-categorical-variables\n",
    "    model.fit(X_train, y_train, **kwargs)\n",
    "    preds = model.predict(X_val)\n",
    "    print(classification_report(y_val,preds))\n",
    "    return mean_absolute_error(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to avoid copy-and-paste\n",
    "def train_and_score_flow(X, y, message=''):\n",
    "    '''Makes train and test split and evaluates the model based on given features (X) and target (y)'''    \n",
    "    # Train and test split\n",
    "    # Remove last rows from the dataset\n",
    "    #test_size = 500\n",
    "    X_train, y_train = X.iloc[:-test_size], y.iloc[:-test_size]\n",
    "    X_val, y_val = X.tail(test_size), y.tail(test_size)\n",
    "    # Categorical encoding\n",
    "    X_train, X_val = pd.get_dummies(X_train), pd.get_dummies(X_val)\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=101)\n",
    "    #random_forest_basic = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "    LogisticRegression_basic = LogisticRegression(solver='warn')\n",
    "    #print(\"Random Forest (drop all NaNs):\")\n",
    "    print(message)\n",
    "    print( score_dataset(LogisticRegression_basic, X_train, X_val, y_train, y_val) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let's already try some methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_basic = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70       584\n",
      "           1       0.55      0.39      0.45       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.58      0.58      1000\n",
      "weighted avg       0.60      0.61      0.60      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.386"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Random Forest :\")\n",
    "score_dataset(random_forest_basic, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel_basic = LogisticRegression(solver='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.85      0.72       584\n",
      "           1       0.58      0.29      0.39       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.60      0.57      0.55      1000\n",
      "weighted avg       0.61      0.62      0.58      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.384"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Logistic regression :\")\n",
    "score_dataset(logmodel_basic, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to evaluate model with different combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.drop(['d_ranging'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop d_ranging:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.91      0.72       584\n",
      "           1       0.52      0.13      0.21       416\n",
      "\n",
      "    accuracy                           0.59      1000\n",
      "   macro avg       0.56      0.52      0.46      1000\n",
      "weighted avg       0.56      0.59      0.51      1000\n",
      "\n",
      "0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_and_score_flow(X1, y, message='drop d_ranging:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'pattern', 'origin', 'ranging', 'd_ranging', 'lower', 'upper',\n",
       "       'middle', 'd_lower', 'd_upper', 'd_middle', 'ema_10', 'ema_200',\n",
       "       'd_ema_10', 'd_ema_200', 'k_15', 'd_15', 'd_k_15', 'd_d_15',\n",
       "       'candle_color', 'volume', 'ntrades', 'tb_volume', 'dist_to_BB'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop volume and n_trades\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.85      0.72       584\n",
      "           1       0.59      0.29      0.39       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.57      0.56      1000\n",
      "weighted avg       0.61      0.62      0.58      1000\n",
      "\n",
      "0.38\n"
     ]
    }
   ],
   "source": [
    "X2 = X.drop(['volume', 'ntrades'], axis=1)\n",
    "train_and_score_flow(X2, y, message='drop volume and n_trades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop d_upper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.85      0.72       584\n",
      "           1       0.58      0.29      0.39       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.60      0.57      0.55      1000\n",
      "weighted avg       0.61      0.62      0.58      1000\n",
      "\n",
      "0.384\n"
     ]
    }
   ],
   "source": [
    "X3 = X.drop(['d_upper'], axis=1)\n",
    "train_and_score_flow(X3, y, message='drop d_upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop volume and n_trades and d_upper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.85      0.72       584\n",
      "           1       0.59      0.29      0.39       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.57      0.56      1000\n",
      "weighted avg       0.61      0.62      0.58      1000\n",
      "\n",
      "0.38\n"
     ]
    }
   ],
   "source": [
    "X4 = X.drop(['volume', 'ntrades', 'd_upper'], axis=1)\n",
    "train_and_score_flow(X2, y, message='drop volume and n_trades and d_upper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_basic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_basic.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = logmodel_basic.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.xticks(np.arange(len(importance)), X_val.columns, rotation=60)\n",
    "plt.ylim([-0.5,0.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logmodel_basic.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[494,  90],\n",
       "       [294, 122]], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(confusion_matrix(y_val,preds))\n",
    "confusion_matrix(y_val,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55619601, 0.44380399],\n",
       "       [0.60742017, 0.39257983],\n",
       "       [0.59127261, 0.40872739],\n",
       "       ...,\n",
       "       [0.43784611, 0.56215389],\n",
       "       [0.56886157, 0.43113843],\n",
       "       [0.50079594, 0.49920406]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_basic.predict_proba(X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different solvers for logostic regression\n",
    "# solvers = {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}\n",
    "# for solver in solvers:\n",
    "#     logmodel_solv = LogisticRegression(solver=solver)\n",
    "#     print(f\"Solver : {solver}\")\n",
    "#     score_dataset(logmodel_solv, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  7.5min finished\n",
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Taken from: https://github.com/finnqiao/bank-logistic/blob/master/bank-logistic-v2.ipynb\n",
    "# https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5\n",
    "\n",
    "# Create first pipeline for base without reducing features.\n",
    "\n",
    "#pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "# pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Create param grid.\n",
    "\n",
    "# param_grid = [\n",
    "#     {'classifier' : [LogisticRegression()],\n",
    "#      'classifier__penalty' : ['l1', 'l2'],\n",
    "#     'classifier__C' : np.logspace(-4, 4, 20),\n",
    "#     'classifier__solver' : ['liblinear']},\n",
    "#     {'classifier' : [RandomForestClassifier()],\n",
    "#     'classifier__n_estimators' : list(range(10,101,10)),\n",
    "#     'classifier__max_features' : list(range(5,31,5))}\n",
    "# ]\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']}\n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "best_clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3792.690190732246, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_estimator_.get_params()['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('classifier',\n",
       "   LogisticRegression(C=3792.690190732246, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                      random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                      warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'classifier': LogisticRegression(C=3792.690190732246, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'classifier__C': 3792.690190732246,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__dual': False,\n",
       " 'classifier__fit_intercept': True,\n",
       " 'classifier__intercept_scaling': 1,\n",
       " 'classifier__l1_ratio': None,\n",
       " 'classifier__max_iter': 100,\n",
       " 'classifier__multi_class': 'warn',\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__solver': 'liblinear',\n",
       " 'classifier__tol': 0.0001,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = best_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.81      0.71       584\n",
      "           1       0.56      0.34      0.43       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.60      0.58      0.57      1000\n",
      "weighted avg       0.60      0.62      0.59      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( classification_report(y_val, preds2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.384"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_val, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_basic = XGBClassifier(n_estimators=1000, learning_rate=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB_basic.fit(X_train, y_train,\n",
    "#              early_stopping_rounds=5, \n",
    "#              eval_set=[(X_val, y_val)], \n",
    "#              verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG Boost :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       584\n",
      "           1       0.56      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"XG Boost :\")\n",
    "score_dataset(XGB_basic, X_train, X_val, y_train, y_val,\n",
    "             early_stopping_rounds=10,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_preds_XGB = XGB_basic.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mae = mean_absolute_error(val_preds_XGB, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"MAE for the XG Boost without categorical features: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset_lgb( X_train, X_val, y_train, y_val, params):\n",
    "    '''Trains a model, makes predictions. \n",
    "    Prints classification report\n",
    "    Returns mean absolute error'''\n",
    "    #Modified from: https://www.kaggle.com/alexisbcook/exercise-categorical-variables\n",
    "    model = LGBMClassifier(objective ='binary',\n",
    "                           boosting ='gbdt', #dart\n",
    "                           n_jobs = -1,\n",
    "                           max_depth = 8,\n",
    "                           num_iterations = 200,\n",
    "                           learning_rate = 0.05,\n",
    "                           **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    #model.fit(X_train, y_train, **kwargs)\n",
    "    preds = model.predict(X_val)\n",
    "    print(classification_report(y_val,preds))\n",
    "    return mean_absolute_error(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_basic = LGBMClassifier(objective ='binary',\n",
    "                       boosting ='gbdt', #dart\n",
    "                       n_jobs = -1,\n",
    "                       max_depth = 8,\n",
    "                       num_iterations = 200,\n",
    "                       learning_rate = 0.05,\n",
    "                        num_leaves = 32\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    boosting='gbdt',\n",
    "    learning_rate = 0.05,\n",
    "    max_depth = 8,\n",
    "    num_leaves = 80,\n",
    "    n_estimators = 400,\n",
    "    bagging_fraction = 0.8,\n",
    "    feature_fraction = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67       584\n",
      "           1       0.54      0.51      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.387"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dataset(LGBM_basic,X_train, X_val, y_train, y_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       584\n",
      "           1       0.53      0.52      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.59      0.59      0.59      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.393"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dataset(lgb,X_train, X_val, y_train, y_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(df, target, train_index, val_index, params):\n",
    "#     model = LGBMClassifier(objective ='binary',\n",
    "#                            boosting ='gbdt', #dart\n",
    "#                            n_jobs = -1,\n",
    "#                            max_depth = 8,\n",
    "#                            num_iterations = 200,\n",
    "#                            learning_rate = 0.05,\n",
    "#                            **params)\n",
    "#     model.fit(df.loc[train_index],bin_target.loc[train_index])\n",
    "#     return metrics.log_loss(target.loc[val_index], \n",
    "#                             model.predict_proba(df.loc[val_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning begins...\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       584\n",
      "           1       0.55      0.55      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.55      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.66       584\n",
      "           1       0.53      0.52      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.59      0.59      0.59      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       584\n",
      "           1       0.56      0.55      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.66       584\n",
      "           1       0.52      0.51      0.52       416\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.59      0.59      0.59      1000\n",
      "weighted avg       0.60      0.60      0.60      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.66       584\n",
      "           1       0.53      0.54      0.54       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       584\n",
      "           1       0.53      0.53      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       584\n",
      "           1       0.53      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.54      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.53      0.53      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.67       584\n",
      "           1       0.54      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       584\n",
      "           1       0.55      0.54      0.55       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       584\n",
      "           1       0.55      0.54      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.54      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.67       584\n",
      "           1       0.54      0.53      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       584\n",
      "           1       0.53      0.51      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68       584\n",
      "           1       0.55      0.54      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       584\n",
      "           1       0.55      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.66       584\n",
      "           1       0.53      0.52      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.59      0.59      0.59      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       584\n",
      "           1       0.54      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.60      0.60      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       584\n",
      "           1       0.53      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       584\n",
      "           1       0.53      0.52      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.66       584\n",
      "           1       0.53      0.53      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       584\n",
      "           1       0.56      0.55      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       584\n",
      "           1       0.54      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.53      0.53      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       584\n",
      "           1       0.53      0.51      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.59      0.59      0.59      1000\n",
      "weighted avg       0.60      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       584\n",
      "           1       0.53      0.53      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       584\n",
      "           1       0.52      0.52      0.52       416\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.59      0.59      0.59      1000\n",
      "weighted avg       0.60      0.60      0.60      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.54      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       584\n",
      "           1       0.54      0.54      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       584\n",
      "           1       0.54      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.55      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.66       584\n",
      "           1       0.52      0.49      0.50       416\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.58      0.58      0.58      1000\n",
      "weighted avg       0.59      0.60      0.59      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.67       584\n",
      "           1       0.54      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.53      0.53      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.67       584\n",
      "           1       0.54      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       584\n",
      "           1       0.53      0.51      0.52       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.59      0.59      0.59      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       584\n",
      "           1       0.56      0.54      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       584\n",
      "           1       0.55      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       584\n",
      "           1       0.55      0.55      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       584\n",
      "           1       0.53      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       584\n",
      "           1       0.54      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68       584\n",
      "           1       0.55      0.54      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.63      0.63      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       584\n",
      "           1       0.51      0.52      0.52       416\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.58      0.58      0.58      1000\n",
      "weighted avg       0.60      0.60      0.60      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       584\n",
      "           1       0.54      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.61      1000\n",
      "   macro avg       0.60      0.60      0.60      1000\n",
      "weighted avg       0.61      0.61      0.61      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       584\n",
      "           1       0.55      0.52      0.53       416\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.62      0.62      1000\n",
      "\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68       584\n",
      "           1       0.55      0.54      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.61      0.61      0.61      1000\n",
      "weighted avg       0.62      0.63      0.63      1000\n",
      "\n",
      "Best evaluation MAE 0.367\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': [60, 80],\n",
    "    'n_estimators': [200, 400], #default class*iteration=2*100\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction' : [0.8, 0.9],  # subsample\n",
    "    'feature_fraction' : [0.8, 0.9],  # colsample_bytree\n",
    "    'reg_alpha': [0.2, 0.6, 0.8],\n",
    "    'reg_lambda': [0.4, 0.6, 0.8]\n",
    "}\n",
    "\n",
    "print('Tuning begins...')\n",
    "best_eval_score = 0\n",
    "for i in range(50):\n",
    "    params = {k: np.random.choice(v) for k, v in param_grid.items()}\n",
    "    #score = evaluate_model(df, bin_target, train_index2, val_index, params)\n",
    "    score = score_dataset_lgb(X_train, X_val, y_train, y_val, params)\n",
    "    if score < best_eval_score or best_eval_score == 0:\n",
    "        best_eval_score = score\n",
    "        best_params = params\n",
    "print(\"Best evaluation MAE\", best_eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 60,\n",
       " 'n_estimators': 400,\n",
       " 'bagging_freq': 1,\n",
       " 'bagging_fraction': 0.9,\n",
       " 'feature_fraction': 0.8,\n",
       " 'reg_alpha': 0.2,\n",
       " 'reg_lambda': 0.6}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_best = LGBMClassifier(objective ='binary',\n",
    "                       boosting ='gbdt', #dart\n",
    "                       n_jobs = -1,\n",
    "                       max_depth = 8,\n",
    "                       num_iterations = 200,\n",
    "                       learning_rate = 0.05,\n",
    "                       num_leaves= 60,\n",
    "                         n_estimators = 400,\n",
    "                         bagging_freq = 1,\n",
    "                         bagging_fraction = 0.9,\n",
    "                         feature_fraction = 0.8,\n",
    "                         reg_alpha = 0.2,\n",
    "                         reg_lambda = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       584\n",
      "           1       0.56      0.55      0.55       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.367"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dataset(lgb_best, X_train, X_val, y_train, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model\n",
    "\n",
    "According to the classification reports, the best model that predicts succesfull trades is happened to be logistic regression.\n",
    "Now, save this model using `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taras\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's train the model again:\n",
    "logmodel_basic = LogisticRegression(solver='warn')\n",
    "logmodel_basic.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre-trained model:\n",
    "# with open('logregression.pickle', 'wb') as f:\n",
    "#     pickle.dump(logmodel_basic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save pre-trained model:\n",
    "# with open('lgb_best.pickle', 'wb') as f:\n",
    "#     pickle.dump(lgb_best, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save pre-trained model:\n",
    "# with open('logregression_vol.pickle', 'wb') as f:\n",
    "#     pickle.dump(logmodel_basic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgboost_vol.pickle', 'wb') as f:\n",
    "    pickle.dump(XGB_basic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pre-trained model:\n",
    "#pickle_in = open('logregression_vol.pickle', 'rb')\n",
    "pickle_in = open('xgboost_vol.pickle', 'rb')\n",
    "logmodel_loaded = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       584\n",
      "           1       0.56      0.53      0.54       416\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.62      0.62      0.62      1000\n",
      "weighted avg       0.63      0.63      0.63      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if loaded model shows the same results:\n",
    "score_dataset(logmodel_loaded, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note. </b> `XGBoost` shows better accuracy in general, but it better predicts loosing trades rather than succesfull. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logmodel_loaded.predict(X_val.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel_loaded.predict( np.array(X_val.iloc[-1]).reshape(1, -1) )[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0 for _ in range(27)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
